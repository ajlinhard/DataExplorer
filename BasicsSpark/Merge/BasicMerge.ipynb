{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta import *\n",
    "\n",
    "# Create a Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Merge Practice\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Merge Practice\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e39ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Merge Practice\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.databricks.delta.properties.defaults.enableColumnDefaults\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82df5e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW action1\n",
    "AS\n",
    "SELECT 'Customer1' as customer, current_timestamp() as insert_dt, 100.00 as amount\n",
    "\"\"\"\n",
    "# Execute the SQL command\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "237a977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+\n",
      "| customer|           insert_dt|amount|\n",
      "+---------+--------------------+------+\n",
      "|Customer1|2025-07-10 15:19:...|100.00|\n",
      "+---------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from action1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07a176",
   "metadata": {},
   "source": [
    "## Create Table and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bdc45eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Catalog: spark_catalog\n"
     ]
    }
   ],
   "source": [
    "# get the current catalog\n",
    "current_catalog = spark.catalog.currentCatalog()\n",
    "print(f\"Current Catalog: {current_catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66ebd5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists new_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac4d149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_sql = \"\"\"\n",
    "CREATE TABLE new_table2 (\n",
    "    customer STRING,\n",
    "    insert_dt TIMESTAMP,\n",
    "    update_dt TIMESTAMP DEFAULT current_timestamp(),\n",
    "    amount DECIMAL(10, 2),\n",
    "    updt_cnt INT DEFAULT 0\n",
    "    --, avg_updt_time_seconds FLOAT GENERATED ALWAYS as (to_unix_timestamp(update_dt) - to_unix_timestamp(insert_dt)) \n",
    ") USING DELTA\n",
    "TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')\n",
    "\"\"\"\n",
    "spark.sql(create_sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "599ec786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+--------+\n",
      "|customer|insert_dt|update_dt|amount|updt_cnt|\n",
      "+--------+---------+---------+------+--------+\n",
      "+--------+---------+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from new_table2\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d398497",
   "metadata": {},
   "source": [
    "### Merge Command\n",
    "The order of merge conditions must be \n",
    "1. MATCHED\n",
    "2. NOT MATCHED BY TARGET\n",
    "3. NOT MATCHED BY SOURCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbc72b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_sql = \"\"\"\n",
    "MERGE INTO new_table2 as t\n",
    "USING action1 as s\n",
    "ON t.customer = s.customer\n",
    "WHEN MATCHED\n",
    "    THEN UPDATE SET\n",
    "        update_dt = current_timestamp(),\n",
    "        t.amount = s.amount,\n",
    "        t.updt_cnt = t.updt_cnt + 1\n",
    "WHEN NOT MATCHED BY TARGET\n",
    "    THEN INSERT (customer, insert_dt, amount)\n",
    "    VALUES (s.customer, s.insert_dt, s.amount)\n",
    "\"\"\"\n",
    "spark.sql(merge_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93a2ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+--------------------------+------+--------+\n",
      "|customer |insert_dt                 |update_dt                 |amount|updt_cnt|\n",
      "+---------+--------------------------+--------------------------+------+--------+\n",
      "|Customer1|2025-07-10 15:36:42.171487|2025-07-10 15:43:01.369167|100.00|3       |\n",
      "+---------+--------------------------+--------------------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from new_table2\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8d590",
   "metadata": {},
   "source": [
    "## Action 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f610f98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW action2\n",
    "AS\n",
    "SELECT 'Customer1' as customer, current_timestamp() as insert_dt, 522.00 as amount\n",
    "UNION ALL\n",
    "SELECT 'Customer2' as customer, current_timestamp() as insert_dt, 33.22 as amount\n",
    "UNION ALL\n",
    "SELECT 'Customer3' as customer, current_timestamp() as insert_dt, 44.44 as amount\n",
    "UNION ALL\n",
    "SELECT 'Customer3' as customer, current_timestamp() as insert_dt, 55.55 as amount\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Execute the SQL command\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07c6fa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge2_sql = \"\"\"\n",
    "MERGE INTO new_Table2 as t\n",
    "USING action2 as s\n",
    "ON t.customer = s.customer\n",
    "WHEN MATCHED\n",
    "    THEN UPDATE SET\n",
    "        update_dt = current_timestamp(),\n",
    "        amount = s.amount,\n",
    "        updt_cnt = t.updt_cnt + 1\n",
    "WHEN NOT MATCHED BY TARGET\n",
    "    THEN INSERT (customer, insert_dt, amount)\n",
    "    VALUES (s.customer, s.insert_dt, s.amount)\n",
    "\"\"\"\n",
    "spark.sql(merge2_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe66759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+-------------------------+------+--------+\n",
      "|customer |insert_dt                 |update_dt                |amount|updt_cnt|\n",
      "+---------+--------------------------+-------------------------+------+--------+\n",
      "|Customer1|2025-07-10 15:36:42.171487|2025-07-10 15:44:31.32122|522.00|4       |\n",
      "|Customer2|2025-07-10 15:44:31.32122 |2025-07-10 15:44:31.32022|33.22 |0       |\n",
      "|Customer3|2025-07-10 15:44:31.32122 |2025-07-10 15:44:31.32022|44.44 |0       |\n",
      "|Customer3|2025-07-10 15:44:31.32122 |2025-07-10 15:44:31.32022|55.55 |0       |\n",
      "+---------+--------------------------+-------------------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from new_table2\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b11f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW action3\n",
    "AS\n",
    "SELECT 'Customer1' as customer\n",
    "UNION\n",
    "SELECT 'Customer2' as customer\n",
    "UNION ALL\n",
    "SELECT 'Customer3' as customer\n",
    "UNION ALL\n",
    "SELECT 'Customer3' as customer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
