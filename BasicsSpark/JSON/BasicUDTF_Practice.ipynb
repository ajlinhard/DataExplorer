{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c459d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UDTFs\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b73b49",
   "metadata": {},
   "source": [
    "## Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53716b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+-----------+\n",
      "| server|  metric_name|metric_value|metric_flag|\n",
      "+-------+-------------+------------+-----------+\n",
      "|server1|    cpu_usage|        80.5|       high|\n",
      "|server1| memory_usage|        65.2|       high|\n",
      "|server1|   disk_usage|        45.8|        low|\n",
      "|server1|network_usage|        12.1|        low|\n",
      "|server2|    cpu_usage|        75.3|       high|\n",
      "|server2| memory_usage|        70.1|       high|\n",
      "|server2|   disk_usage|        50.2|       high|\n",
      "|server2|network_usage|        15.5|       high|\n",
      "+-------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stacks\n",
    "# Sample wide data\n",
    "metrics_data = [\n",
    "    (\"server1\", 80.5, 65.2, 45.8, 12.1),\n",
    "    (\"server2\", 75.3, 70.1, 50.2, 15.5)\n",
    "]\n",
    "metrics_df = spark.createDataFrame(metrics_data, [\"server\", \"cpu\", \"memory\", \"disk\", \"network\"])\n",
    "\n",
    "# Using stack() in SQL parameters are number of pairs then value pairs\n",
    "metrics_df.createOrReplaceTempView(\"metrics\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT server, metric_name, metric_value, metric_flag\n",
    "    FROM metrics\n",
    "    LATERAL VIEW stack(4, \n",
    "        'cpu_usage', cpu, case when cpu < 70 then 'low' else 'high' end,\n",
    "        'memory_usage', memory, case when memory < 60 then 'low' else 'high' end,\n",
    "        'disk_usage', disk, case when disk < 50 then 'low' else 'high' end,\n",
    "        'network_usage', network, case when network < 15 then 'low' else 'high' end\n",
    "    ) stacked AS metric_name, metric_value, metric_flag\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc50a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+\n",
      "| server|  metric_name|metric_value|\n",
      "+-------+-------------+------------+\n",
      "|server1|    cpu_usage|        80.5|\n",
      "|server1| memory_usage|        65.2|\n",
      "|server1|   disk_usage|        45.8|\n",
      "|server1|network_usage|        12.1|\n",
      "|server2|    cpu_usage|        75.3|\n",
      "|server2| memory_usage|        70.1|\n",
      "|server2|   disk_usage|        50.2|\n",
      "|server2|network_usage|        15.5|\n",
      "+-------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API equivalent (more verbose)\n",
    "from pyspark.sql.functions import lit, array, struct, explode, col\n",
    "metrics_df.select(\n",
    "    \"server\",\n",
    "    explode(array(\n",
    "        struct(lit(\"cpu_usage\").alias(\"metric_name\"), col(\"cpu\").alias(\"metric_value\")),\n",
    "        struct(lit(\"memory_usage\").alias(\"metric_name\"), col(\"memory\").alias(\"metric_value\")),\n",
    "        struct(lit(\"disk_usage\").alias(\"metric_name\"), col(\"disk\").alias(\"metric_value\")),\n",
    "        struct(lit(\"network_usage\").alias(\"metric_name\"), col(\"network\").alias(\"metric_value\"))\n",
    "    )).alias(\"metric\")\n",
    ").select(\"server\", \"metric.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21c9ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+----+-------+-------------+------------+---+\n",
      "|server |cpu |memory|disk|network|metric_name  |metric_value|sdf|\n",
      "+-------+----+------+----+-------+-------------+------------+---+\n",
      "|server1|80.5|65.2  |45.8|12.1   |cpu_usage    |80.5        |1  |\n",
      "|server1|80.5|65.2  |45.8|12.1   |memory_usage |65.2        |1  |\n",
      "|server1|80.5|65.2  |45.8|12.1   |disk_usage   |45.8        |1  |\n",
      "|server1|80.5|65.2  |45.8|12.1   |network_usage|12.1        |1  |\n",
      "|server2|75.3|70.1  |50.2|15.5   |cpu_usage    |75.3        |1  |\n",
      "|server2|75.3|70.1  |50.2|15.5   |memory_usage |70.1        |1  |\n",
      "|server2|75.3|70.1  |50.2|15.5   |disk_usage   |50.2        |1  |\n",
      "|server2|75.3|70.1  |50.2|15.5   |network_usage|15.5        |1  |\n",
      "+-------+----+------+----+-------+-------------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stack\n",
    "# Using stack() in DataFrame API\n",
    "metrics_df.select('*', \n",
    "        stack(lit(4),\n",
    "            lit(\"cpu_usage\"), col(\"cpu\"), lit(1), \n",
    "            lit(\"memory_usage\"), col(\"memory\"), lit(1),\n",
    "            lit(\"disk_usage\"), col(\"disk\"), lit(1),\n",
    "            lit(\"network_usage\"), col(\"network\"), lit(1)\n",
    "        ).alias(\"metric_name\", \"metric_value\", \"sdf\")\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd902f",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "079cc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "json_data = [\n",
    "    (\"Alice\", '{\"age\": 25, \"city\": \"NYC\", \"salary\": 50000}'),\n",
    "    (\"Bob\", '{\"age\": 30, \"city\": \"LA\", \"salary\": 60000}')\n",
    "]\n",
    "json_df = spark.createDataFrame(json_data, [\"name\", \"json_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723ebc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+------+-----------+\n",
      "|name |age|city|salary|always_null|\n",
      "+-----+---+----+------+-----------+\n",
      "|Alice|25 |NYC |50000 |NULL       |\n",
      "|Bob  |30 |LA  |60000 |NULL       |\n",
      "+-----+---+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"always_null\", IntegerType(), True)\n",
    "])\n",
    "df_from = json_df.select('*', from_json(col('json_str'), json_schema).alias('json_data'))\n",
    "\n",
    "df_from.select('name', 'json_data.*').show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ba32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------+---+----+------+-----------+\n",
      "|name |json_str                                   |age|city|salary|always_null|\n",
      "+-----+-------------------------------------------+---+----+------+-----------+\n",
      "|Alice|{\"age\": 25, \"city\": \"NYC\", \"salary\": 50000}|25 |NYC |50000 |NULL       |\n",
      "|Bob  |{\"age\": 30, \"city\": \"LA\", \"salary\": 60000} |30 |LA  |60000 |NULL       |\n",
      "+-----+-------------------------------------------+---+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ls_col = ['age', 'city', 'salary', 'always_null']\n",
    "df_tuple = json_df.select('*', json_tuple(col('json_str'), *ls_col).alias(*ls_col))\n",
    "\n",
    "df_tuple.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65746e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ls_col = ['age', 'city', 'salary', 'always_null']\n",
    "df_tuple_unordered = json_df.select('*', json_tuple(col('json_str'), *ls_col).alias(*ls_col))\n",
    "\n",
    "df_tuple_unordered.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e073871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- details: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      "\n",
      "+-------+---+----+------------------------------------------------------------------------------+\n",
      "|name   |age|city|details                                                                       |\n",
      "+-------+---+----+------------------------------------------------------------------------------+\n",
      "|Alice  |25 |NYC |{department -> NULL, salary -> 50000, job_title -> NULL, bonus -> 5000}       |\n",
      "|Bob    |30 |LA  |{department -> NULL, salary -> 60000, job_title -> NULL, bonus -> 6000}       |\n",
      "|Charlie|28 |SF  |{department -> NULL, salary -> 55000, special_note -> NULL, job_title -> NULL}|\n",
      "+-------+---+----+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Alice\", 25, \"NYC\", {\"salary\": 50000, \"bonus\": 5000, \"department\": \"Engineering\", \"job_title\": \"Engineer\"}),\n",
    "    (\"Bob\", 30, \"LA\", {\"salary\": 60000, \"bonus\": 6000, \"department\": \"Marketing\", \"job_title\": \"Manager\"}),\n",
    "    (\"Charlie\", 28, \"SF\", {\"salary\": 55000, \"department\": \"Sales\", \"job_title\": \"Salesperson\", \"special_note\": \"Top performer\"}),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"details\"])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebe288b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1503060548.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[35], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    (\"Alice\", 25, \"NYC\", (\"salary\": 50000, \"bonus\": 5000, \"department\": \"Engineering\", \"job_title\": \"Engineer\")),\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Alice\", 25, \"NYC\", (\"salary\": 50000, \"bonus\": 5000, \"department\": \"Engineering\", \"job_title\": \"Engineer\")),\n",
    "    (\"Bob\", 30, \"LA\", (\"salary\": 60000, \"bonus\": 6000, \"department\": \"Marketing\", \"job_title\": \"Manager\")),\n",
    "    (\"Charlie\", 28, \"SF\", (\"salary\": 55000, \"department\": \"Sales\", \"job_title\": \"Salesperson\", \"special_note\": \"Top performer\")),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"details\"])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1b44c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|map_keys(details)                            |\n",
      "+---------------------------------------------+\n",
      "|[department, salary, job_title, bonus]       |\n",
      "|[department, salary, job_title, bonus]       |\n",
      "|[department, salary, special_note, job_title]|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(map_keys(\"details\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf8fea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|department|salary|\n",
      "+----------+------+\n",
      "|NULL      |50000 |\n",
      "|NULL      |60000 |\n",
      "|NULL      |55000 |\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"details.department\").alias(\"department\"),\n",
    "          col(\"details.salary\").alias(\"salary\"),\n",
    ").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b30303fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|key       |\n",
      "+----------+\n",
      "|department|\n",
      "|salary    |\n",
      "|job_title |\n",
      "|bonus     |\n",
      "|department|\n",
      "|salary    |\n",
      "|job_title |\n",
      "|bonus     |\n",
      "|department|\n",
      "|salary    |\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys, explode, collect_set\n",
    "\n",
    "# Get all unique keys across all maps\n",
    "df_keys = df.select(explode(map_keys(\"details\")).alias(\"key\"))\n",
    "df_keys.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "718320bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+\n",
      "|keys                                                |\n",
      "+----------------------------------------------------+\n",
      "|[special_note, salary, department, job_title, bonus]|\n",
      "+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_key_set = df_keys.select(collect_set(\"key\").alias(\"keys\"))\n",
    "df_key_set.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "859d16fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> ['special_note', 'salary', 'department', 'job_title', 'bonus']\n"
     ]
    }
   ],
   "source": [
    "all_keys = df_key_set.collect()[0][\"keys\"]\n",
    "print(type(all_keys), all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e028cc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Expression: ['name', \"details['bonus'] as bonus\", \"details['department'] as department\", \"details['job_title'] as job_title\", \"details['salary'] as salary\", \"details['special_note'] as special_note\"]\n",
      "+-------+-----+----------+---------+------+------------+\n",
      "|   name|bonus|department|job_title|salary|special_note|\n",
      "+-------+-----+----------+---------+------+------------+\n",
      "|  Alice| 5000|      NULL|     NULL| 50000|        NULL|\n",
      "|    Bob| 6000|      NULL|     NULL| 60000|        NULL|\n",
      "|Charlie| NULL|      NULL|     NULL| 55000|        NULL|\n",
      "+-------+-----+----------+---------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create columns dynamically\n",
    "select_expr = [\"name\"] + [f\"details['{key}'] as {key}\" for key in sorted(all_keys)]\n",
    "print(\"Select Expression:\", select_expr)\n",
    "result = df.selectExpr(*select_expr)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d17ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+---------+------+------------+\n",
      "|name   |bonus|department|job_title|salary|special_note|\n",
      "+-------+-----+----------+---------+------+------------+\n",
      "|Alice  |5000 |NULL      |NULL     |50000 |NULL        |\n",
      "|Bob    |6000 |NULL      |NULL     |60000 |NULL        |\n",
      "|Charlie|NULL |NULL      |NULL     |55000 |NULL        |\n",
      "+-------+-----+----------+---------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('name', \n",
    "              \"details['bonus'] as bonus\", \n",
    "              \"details['department'] as department\", \n",
    "              \"details['job_title'] as job_title\", \n",
    "              \"details['salary'] as salary\", \"details['special_note'] as special_note\").show(truncate=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
