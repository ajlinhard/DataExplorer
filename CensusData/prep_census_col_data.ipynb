{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@Andrew_Dev:62663\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m s_raw_root_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataSamples\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataExplorer\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCensus\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m s_spark_file_server_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSpark_Data_Test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrep Census Data\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.warehouse.dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, s_spark_file_server_root) \\\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39menableHiveSupport() \\\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[1;32mc:\\Users\\dalej\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\dalej\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\dalej\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[0;32m    206\u001b[0m         sparkHome,\n\u001b[0;32m    207\u001b[0m         pyFiles,\n\u001b[0;32m    208\u001b[0m         environment,\n\u001b[0;32m    209\u001b[0m         batchSize,\n\u001b[0;32m    210\u001b[0m         serializer,\n\u001b[0;32m    211\u001b[0m         conf,\n\u001b[0;32m    212\u001b[0m         jsc,\n\u001b[0;32m    213\u001b[0m         profiler_cls,\n\u001b[0;32m    214\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\dalej\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39m_jconf)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\Users\\dalej\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mJavaSparkContext(jconf)\n",
      "File \u001b[1;32mc:\\Users\\dalej\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\dalej\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@Andrew_Dev:62663\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "s_raw_root_path = r'F:\\DataSamples\\DataExplorer\\Census'\n",
    "s_spark_file_server_root = r'F:\\Spark_Data_Test'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Prep Census Data') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", s_spark_file_server_root) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+--------+--------+------------+--------+--------+------+-------+---------+-----------+\n",
      "|name           |rank|count   |prop100k|cum_prop100k|pctwhite|pctblack|pctapi|pctaian|pct2prace|pcthispanic|\n",
      "+---------------+----+--------+--------+------------+--------+--------+------+-------+---------+-----------+\n",
      "|ALL OTHER NAMES|0   |29312001|9936.97 |9936.97     |66.65   |8.53    |7.97  |0.86   |2.32     |13.67      |\n",
      "|SMITH          |1   |2442977 |828.19  |828.19      |70.9    |23.11   |0.5   |0.89   |2.19     |2.4        |\n",
      "|JOHNSON        |2   |1932812 |655.24  |1483.42     |58.97   |34.63   |0.54  |0.94   |2.56     |2.36       |\n",
      "|WILLIAMS       |3   |1625252 |550.97  |2034.39     |45.75   |47.68   |0.46  |0.82   |2.81     |2.49       |\n",
      "|BROWN          |4   |1437026 |487.16  |2521.56     |57.95   |35.6    |0.51  |0.87   |2.55     |2.52       |\n",
      "|JONES          |5   |1425470 |483.24  |3004.8      |55.19   |38.48   |0.44  |1      |2.61     |2.29       |\n",
      "|GARCIA         |6   |1166120 |395.32  |3400.12     |5.38    |0.45    |1.41  |0.47   |0.26     |92.03      |\n",
      "|MILLER         |7   |1161437 |393.74  |3793.86     |84.11   |10.76   |0.54  |0.66   |1.77     |2.17       |\n",
      "|DAVIS          |8   |1116357 |378.45  |4172.31     |62.2    |31.6    |0.49  |0.82   |2.45     |2.44       |\n",
      "|RODRIGUEZ      |9   |1094924 |371.19  |4543.5      |4.75    |0.54    |0.57  |0.18   |0.18     |93.77      |\n",
      "+---------------+----+--------+--------+------------+--------+--------+------+-------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Files\n",
    "df_census_surname = spark.read \\\n",
    "    .options(header='true', delimiter=',', inferSchema=True) \\\n",
    "    .csv(os.path.join(s_raw_root_path, 'Census_Surnames\\\\Names_2010Census.csv'))\n",
    "\n",
    "df_census_surname.orderBy('rank', ascending=True).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160975"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_census_surname.select(max('rank')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- rank: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- prop100k: double (nullable = true)\n",
      " |-- cum_prop100k: double (nullable = true)\n",
      " |-- pctwhite: string (nullable = true)\n",
      " |-- pctblack: string (nullable = true)\n",
      " |-- pctapi: string (nullable = true)\n",
      " |-- pctaian: string (nullable = true)\n",
      " |-- pct2prace: string (nullable = true)\n",
      " |-- pcthispanic: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_census_surname.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Names by Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+--------+------------+--------+--------+------+-------+---------+-----------+------------+-----------+\n",
      "|name     |rank|count |prop100k|cum_prop100k|pctwhite|pctblack|pctapi|pctaian|pct2prace|pcthispanic|first_letter|letter_rank|\n",
      "+---------+----+------+--------+------------+--------+--------+------+-------+---------+-----------+------------+-----------+\n",
      "|ANDERSON |15  |784404|265.92  |6375.92     |75.17   |18.93   |0.61  |0.74   |2.11     |2.44       |A           |1          |\n",
      "|ALLEN    |33  |482607|163.61  |10190.14    |67.59   |26.17   |0.54  |0.86   |2.38     |2.47       |A           |2          |\n",
      "|ADAMS    |42  |427865|145.05  |11534.72    |74.02   |19.9    |0.56  |0.79   |2.16     |2.57       |A           |3          |\n",
      "|ALVAREZ  |92  |233983|79.32   |16697.86    |5.18    |0.6     |1.16  |0.38   |0.23     |92.45      |A           |4          |\n",
      "|ALEXANDER|118 |204621|69.37   |18637.23    |58.19   |34.04   |1.24  |0.7    |2.66     |3.17       |A           |5          |\n",
      "|AGUILAR  |134 |186512|63.23   |19691.45    |4.03    |0.26    |1.64  |0.52   |0.23     |93.32      |A           |6          |\n",
      "|ARNOLD   |201 |138893|47.09   |23322.91    |81.27   |13.34   |0.57  |0.55   |1.87     |2.41       |A           |7          |\n",
      "|ARMSTRONG|213 |135044|45.78   |23879.56    |72.39   |21.01   |0.58  |0.93   |2.31     |2.79       |A           |8          |\n",
      "|ANDREWS  |218 |133799|45.36   |24107.1     |71.78   |21.58   |0.78  |1.09   |2.2      |2.57       |A           |9          |\n",
      "|ALVARADO |219 |133501|45.26   |24152.35    |4.29    |0.33    |0.53  |0.23   |0.2      |94.42      |A           |10         |\n",
      "+---------+----+------+--------+------------+--------+--------+------+-------+---------+-----------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+----+-------+--------+------------+--------+--------+------+-------+---------+-----------+------------+-----------+\n",
      "|name     |rank|count  |prop100k|cum_prop100k|pctwhite|pctblack|pctapi|pctaian|pct2prace|pcthispanic|first_letter|letter_rank|\n",
      "+---------+----+-------+--------+------------+--------+--------+------+-------+---------+-----------+------------+-----------+\n",
      "|ANDERSON |15  |784404 |265.92  |6375.92     |75.17   |18.93   |0.61  |0.74   |2.11     |2.44       |A           |1          |\n",
      "|BROWN    |4   |1437026|487.16  |2521.56     |57.95   |35.6    |0.51  |0.87   |2.55     |2.52       |B           |1          |\n",
      "|CLARK    |27  |562679 |190.75  |9136.1      |74.65   |19.02   |0.54  |1.01   |2.19     |2.58       |C           |1          |\n",
      "|DAVIS    |8   |1116357|378.45  |4172.31     |62.2    |31.6    |0.49  |0.82   |2.45     |2.44       |D           |1          |\n",
      "|EVANS    |53  |355593 |120.55  |12975.22    |68.39   |25.72   |0.47  |0.74   |2.3      |2.39       |E           |1          |\n",
      "|FLORES   |40  |433969 |147.12  |11243.84    |4.87    |0.42    |2.08  |0.34   |0.36     |91.94      |F           |1          |\n",
      "|GARCIA   |6   |1166120|395.32  |3400.12     |5.38    |0.45    |1.41  |0.47   |0.26     |92.03      |G           |1          |\n",
      "|HERNANDEZ|11  |1043281|353.68  |5256.58     |3.79    |0.36    |0.6   |0.19   |0.16     |94.89      |H           |1          |\n",
      "|INGRAM   |484 |69345  |23.51   |32555.76    |61.45   |32.98   |0.36  |0.54   |2.25     |2.42       |I           |1          |\n",
      "|JOHNSON  |2   |1932812|655.24  |1483.42     |58.97   |34.63   |0.54  |0.94   |2.56     |2.36       |J           |1          |\n",
      "+---------+----+-------+--------+------------+--------+--------+------+-------+---------+-----------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_by_letter = df_census_surname.filter(col('name') != 'ALL OTHER NAMES') \\\n",
    "    .withColumn('first_letter', F.substring(col('name'),1,1)) \\\n",
    "    .withColumn('letter_rank', row_number().over(Window.partitionBy('first_letter').orderBy('cum_prop100k')))\n",
    "\n",
    "df_by_letter.filter(col('first_letter') == 'A').orderBy('letter_rank', ascending=True).show(10, truncate=False)\n",
    "df_by_letter.filter(col('letter_rank') == 1).orderBy('first_letter', ascending=True).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a upper bound for cumulative distribution function\n",
    "df_census = df_census_surname.filter(col('name') != 'ALL OTHER NAMES') \\\n",
    "    .withColumn('unqiue_rank', row_number().over(Window.orderBy('cum_prop100k'))) \\\n",
    "    .withColumn('cum_prop100k', (col('cum_prop100k') * 100).cast(IntegerType()))\n",
    "\n",
    "df_join = df_census.alias('high').join(df_census.alias('low'), on= col('low.unqiue_rank') == col('high.unqiue_rank') - 1, how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------------+-----------------+\n",
      "|     name|unqiue_rank|cum_prop100k_low|cum_prop100k_high|\n",
      "+---------+-----------+----------------+-----------------+\n",
      "|    SMITH|          1|               0|            82819|\n",
      "|  JOHNSON|          2|           82819|           148342|\n",
      "| WILLIAMS|          3|          148342|           203439|\n",
      "|    BROWN|          4|          203439|           252156|\n",
      "|    JONES|          5|          252156|           300480|\n",
      "|   GARCIA|          6|          300480|           340012|\n",
      "|   MILLER|          7|          340012|           379386|\n",
      "|    DAVIS|          8|          379386|           417231|\n",
      "|RODRIGUEZ|          9|          417231|           454350|\n",
      "| MARTINEZ|         10|          454350|           490289|\n",
      "+---------+-----------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----------+----------------+-----------------+\n",
      "|       name|unqiue_rank|cum_prop100k_low|cum_prop100k_high|\n",
      "+-----------+-----------+----------------+-----------------+\n",
      "|      OTHER|     162254|         9006303|         10000000|\n",
      "|    DORIOTT|     162253|         9006299|          9006303|\n",
      "|     DONLEA|     162252|         9006296|          9006299|\n",
      "|      DOKAS|     162251|         9006293|          9006296|\n",
      "|  DIETZMANN|     162250|         9006289|          9006293|\n",
      "|     DOBBEN|     162249|         9006286|          9006289|\n",
      "|     DOBRON|     162248|         9006283|          9006286|\n",
      "| DITERLIZZI|     162247|         9006279|          9006283|\n",
      "| DIFILLIPPO|     162246|         9006276|          9006279|\n",
      "|DONNERMEYER|     162245|         9006272|          9006276|\n",
      "+-----------+-----------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = df_join.select(\n",
    "    coalesce(col('high.name'),lit('OTHER')).alias('name'),\n",
    "    coalesce(col('high.unqiue_rank'),col('low.unqiue_rank')+1).alias('unqiue_rank'),\n",
    "    ifnull(col('low.cum_prop100k'), lit(0)).alias('cum_prop100k_low'),\n",
    "    ifnull(col('high.cum_prop100k'), lit(10_000_000)).alias('cum_prop100k_high'),\n",
    ")\n",
    "\n",
    "df_join.orderBy(col('cum_prop100k_low'), ascending=True\n",
    ").show(10)\n",
    "df_join.orderBy(col('cum_prop100k_low'), ascending=False\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.withColumnRenamed('name','last_name').withColumnRenamed('cum_prop100k_low','profile_lower_bound').withColumnRenamed('cum_prop100k_high','profile_upper_bound') \\\n",
    "    .write.mode('overwrite').parquet(os.path.join(s_spark_file_server_root, 'census_surname_bounds.parquet'))\n",
    "# df_join.write.mode('overwrite').saveAsTable('census_surname_bounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+-------------------+\n",
      "|last_name|unqiue_rank|profile_lower_bound|profile_upper_bound|\n",
      "+---------+-----------+-------------------+-------------------+\n",
      "|SMITH    |1          |0                  |82819              |\n",
      "|JOHNSON  |2          |82819              |148342             |\n",
      "|WILLIAMS |3          |148342             |203439             |\n",
      "|BROWN    |4          |203439             |252156             |\n",
      "|JONES    |5          |252156             |300480             |\n",
      "|GARCIA   |6          |300480             |340012             |\n",
      "|MILLER   |7          |340012             |379386             |\n",
      "|DAVIS    |8          |379386             |417231             |\n",
      "|RODRIGUEZ|9          |417231             |454350             |\n",
      "|MARTINEZ |10         |454350             |490289             |\n",
      "+---------+-----------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "162254\n",
      "162254\n"
     ]
    }
   ],
   "source": [
    "# reload test\n",
    "df_first_names = spark.read.parquet(r'F:\\Spark_Data_Test\\census_surname_bounds.parquet')\n",
    "df_first_names.show(10, truncate=False)\n",
    "# Check count matches on reload\n",
    "print(df_first_names.count())\n",
    "print(df_join.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.withColumnRenamed('name','first_name').withColumnRenamed('cum_prop100k_low','profile_lower_bound').withColumnRenamed('cum_prop100k_high','profile_upper_bound') \\\n",
    "    .write.mode('overwrite').parquet(os.path.join(s_spark_file_server_root, 'census_firstname_bounds.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "+--------+\n",
      "|name_int|\n",
      "+--------+\n",
      "| 5713313|\n",
      "|  849722|\n",
      "| 3755173|\n",
      "| 8112786|\n",
      "| 1929916|\n",
      "| 3493837|\n",
      "| 1589411|\n",
      "| 8082121|\n",
      "| 6311311|\n",
      "| 7878448|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.DataCreator.DataGenerators.PyData import PyData\n",
    "\n",
    "li_name_lkp = PyData.random_ints(1000, 1, 10_000_000)\n",
    "print(type(li_name_lkp))\n",
    "schema = StructType([StructField('name_int', IntegerType(), False)])\n",
    "df_new_names = spark.createDataFrame(zip(li_name_lkp), schema)\n",
    "df_new_names.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+----------------+-----------------+\n",
      "|name_int|       name|unqiue_rank|cum_prop100k_low|cum_prop100k_high|\n",
      "+--------+-----------+-----------+----------------+-----------------+\n",
      "| 5713313|  MCFARLANE|       3795|         5713182|          5713498|\n",
      "|  849722|      WHITE|         24|          830208|           852599|\n",
      "| 3755173|    ENGLISH|        748|         3754501|          3756074|\n",
      "| 8112786|STANCHFIELD|      40510|         8112777|          8112795|\n",
      "| 1929916|     BRYANT|        128|         1924103|          1930639|\n",
      "| 3493837|     DECKER|        597|         3492033|          3493951|\n",
      "| 1589411|     CHAVEZ|         83|         1587366|          1595871|\n",
      "| 8082121|      JOBST|      38877|         8082113|          8082132|\n",
      "| 6311311|       SENN|       6273|         6311249|          6311434|\n",
      "| 7878448|     ELISON|      29856|         7878437|          7878463|\n",
      "+--------+-----------+-----------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_name_w_str = df_new_names.join(df_join, on= (df_new_names.name_int > df_join.cum_prop100k_low) & (df_new_names.name_int <= df_join.cum_prop100k_high), how='left')\n",
    "df_new_name_w_str.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|count(1)|count(DISTINCT name)|\n",
      "+--------+--------------------+\n",
      "|1000    |766                 |\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_name_w_str.select(count('*'), countDistinct('name')).show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|name     |name_cnt|\n",
      "+---------+--------+\n",
      "|OTHER    |82      |\n",
      "|JOHNSON  |10      |\n",
      "|SMITH    |9       |\n",
      "|ANDERSON |9       |\n",
      "|DAVIS    |8       |\n",
      "|BROWN    |8       |\n",
      "|JONES    |6       |\n",
      "|GARCIA   |6       |\n",
      "|MOORE    |6       |\n",
      "|HERNANDEZ|6       |\n",
      "+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_name_w_str.groupBy('name').agg(count('*').alias('name_cnt')).orderBy('name_cnt', ascending=False).show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
